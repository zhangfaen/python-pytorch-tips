{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /root/miniconda3/envs/openmmlab:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main  \n",
      "_openmp_mutex             5.1                       1_gnu  \n",
      "asttokens                 2.0.5              pyhd3eb1b0_0  \n",
      "backcall                  0.2.0              pyhd3eb1b0_0  \n",
      "blas                      1.0                         mkl  \n",
      "bzip2                     1.0.8                h7b6447c_0  \n",
      "ca-certificates           2022.07.19           h06a4308_0  \n",
      "certifi                   2022.9.14        py38h06a4308_0  \n",
      "cudatoolkit               11.1.74              h6bb024c_0    nvidia\n",
      "debugpy                   1.5.1            py38h295c915_0  \n",
      "decorator                 5.1.1              pyhd3eb1b0_0  \n",
      "entrypoints               0.4              py38h06a4308_0  \n",
      "executing                 0.8.3              pyhd3eb1b0_0  \n",
      "ffmpeg                    4.2.2                h20bf706_0  \n",
      "freetype                  2.11.0               h70c0345_0  \n",
      "giflib                    5.2.1                h7b6447c_0  \n",
      "gmp                       6.2.1                h295c915_3  \n",
      "gnutls                    3.6.15               he1e5248_0  \n",
      "intel-openmp              2021.4.0          h06a4308_3561  \n",
      "ipykernel                 6.15.2           py38h06a4308_0  \n",
      "ipython                   8.4.0            py38h06a4308_0  \n",
      "jedi                      0.18.1           py38h06a4308_1  \n",
      "jpeg                      9b                   h024ee3a_2  \n",
      "jupyter_client            7.3.5            py38h06a4308_0  \n",
      "jupyter_core              4.10.0           py38h06a4308_0  \n",
      "lame                      3.100                h7b6447c_0  \n",
      "lcms2                     2.12                 h3be6417_0  \n",
      "ld_impl_linux-64          2.38                 h1181459_1  \n",
      "libffi                    3.3                  he6710b0_2  \n",
      "libgcc-ng                 11.2.0               h1234567_1  \n",
      "libgomp                   11.2.0               h1234567_1  \n",
      "libidn2                   2.3.2                h7f8727e_0  \n",
      "libopus                   1.3.1                h7b6447c_0  \n",
      "libpng                    1.6.37               hbc83047_0  \n",
      "libsodium                 1.0.18               h7b6447c_0  \n",
      "libstdcxx-ng              11.2.0               h1234567_1  \n",
      "libtasn1                  4.16.0               h27cfd23_0  \n",
      "libtiff                   4.1.0                h2733197_1  \n",
      "libunistring              0.9.10               h27cfd23_0  \n",
      "libuv                     1.40.0               h7b6447c_0  \n",
      "libvpx                    1.7.0                h439df22_0  \n",
      "libwebp                   1.2.0                h89dd481_0  \n",
      "lz4-c                     1.9.3                h295c915_1  \n",
      "matplotlib-inline         0.1.6            py38h06a4308_0  \n",
      "mkl                       2021.4.0           h06a4308_640  \n",
      "mkl-service               2.4.0            py38h7f8727e_0  \n",
      "mkl_fft                   1.3.1            py38hd3c417c_0  \n",
      "mkl_random                1.2.2            py38h51133e4_0  \n",
      "ncurses                   6.3                  h5eee18b_3  \n",
      "nest-asyncio              1.5.5            py38h06a4308_0  \n",
      "nettle                    3.7.3                hbbd107a_1  \n",
      "ninja                     1.10.2               h06a4308_5  \n",
      "ninja-base                1.10.2               hd09550d_5  \n",
      "numpy                     1.23.1           py38h6c91a56_0  \n",
      "numpy-base                1.23.1           py38ha15fc14_0  \n",
      "openh264                  2.1.1                h4ff587b_0  \n",
      "openssl                   1.1.1q               h7f8727e_0  \n",
      "packaging                 21.3               pyhd3eb1b0_0  \n",
      "parso                     0.8.3              pyhd3eb1b0_0  \n",
      "pexpect                   4.8.0              pyhd3eb1b0_3  \n",
      "pickleshare               0.7.5           pyhd3eb1b0_1003  \n",
      "pillow                    9.2.0            py38hace64e9_1  \n",
      "pip                       22.2.2           py38h06a4308_0  \n",
      "prompt-toolkit            3.0.20             pyhd3eb1b0_0  \n",
      "psutil                    5.9.0            py38h5eee18b_0  \n",
      "ptyprocess                0.7.0              pyhd3eb1b0_2  \n",
      "pure_eval                 0.2.2              pyhd3eb1b0_0  \n",
      "pygments                  2.11.2             pyhd3eb1b0_0  \n",
      "pyparsing                 3.0.9            py38h06a4308_0  \n",
      "python                    3.8.13               h12debd9_0  \n",
      "python-dateutil           2.8.2              pyhd3eb1b0_0  \n",
      "pytorch                   1.8.2           py3.8_cuda11.1_cudnn8.0.5_0    pytorch-lts\n",
      "pyzmq                     23.2.0           py38h6a678d5_0  \n",
      "readline                  8.1.2                h7f8727e_1  \n",
      "setuptools                63.4.1           py38h06a4308_0  \n",
      "six                       1.16.0             pyhd3eb1b0_1  \n",
      "sqlite                    3.39.3               h5082296_0  \n",
      "stack_data                0.2.0              pyhd3eb1b0_0  \n",
      "tk                        8.6.12               h1ccaba5_0  \n",
      "torchaudio                0.8.2                      py38    pytorch-lts\n",
      "torchvision               0.9.2                py38_cu111    pytorch-lts\n",
      "tornado                   6.2              py38h5eee18b_0  \n",
      "traitlets                 5.1.1              pyhd3eb1b0_0  \n",
      "typing_extensions         4.3.0            py38h06a4308_0  \n",
      "wcwidth                   0.2.5              pyhd3eb1b0_0  \n",
      "wheel                     0.37.1             pyhd3eb1b0_0  \n",
      "x264                      1!157.20191217       h7b6447c_0  \n",
      "xz                        5.2.6                h5eee18b_0  \n",
      "zeromq                    4.3.4                h2531618_0  \n",
      "zlib                      1.2.12               h5eee18b_3  \n",
      "zstd                      1.4.9                haebb681_0  \n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand behavior of Batch Normalization of pytorch 1.8.2, below is the document from pytorch\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm2d#torch.nn.BatchNorm2d\n",
    "\n",
    "Backend C++ source file:\n",
    "https://github.com/pytorch/pytorch/blob/420b37f3c67950ed93cd8aa7a12e673fcfc5567b/aten/src/ATen/native/Normalization.cpp#L61-L126 \n",
    "\n",
    "torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)[SOURCE]\n",
    "\n",
    "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .\n",
    "\n",
    "y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
    "\n",
    "The mean and standard-deviation are calculated per-dimension over the mini-batches and \\gammaγ and \\betaβ are learnable parameter vectors of size C (where C is the input size). By default, the elements of \\gammaγ are set to 1 and the elements of \\betaβ are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).\n",
    "\n",
    "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.\n",
    "\n",
    "If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.\n",
    "\n",
    "NOTE\n",
    "\n",
    "This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is \\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t \n",
    "​\n",
    "is the new observed value.\n",
    "\n",
    "Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it’s common terminology to call this Spatial Batch Normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after many batches and before call eval(), running_mean and running_var : tensor([1.5000, 1.9999]) tensor([0.2857, 1.1429]) 0.1\n",
      "after many batches and after call eval(), running_mean and running_var : tensor([1.5000, 1.9999]) tensor([0.2857, 1.1429]) 0.1\n",
      "BatchNorm2d eval input1:  tensor([[[[    -0.9353,     -0.9353],\n",
      "          [    -0.9353,     -0.9353]],\n",
      "\n",
      "         [[     0.0000,      0.0000],\n",
      "          [     0.0000,      0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "m = nn.BatchNorm2d(2, affine = False, momentum = 0.1, track_running_stats = True)\n",
    "input1 = torch.tensor(\n",
    "    [[[[1., 1.],\n",
    "       [1., 1.]],\n",
    "      [[1., 1.],\n",
    "       [1., 1.]]], \n",
    "     [[[2., 2.], \n",
    "       [2., 2.]], \n",
    "      [[3., 3.], \n",
    "       [3., 3.]]]])\n",
    "for i in range(100):\n",
    "    # print(m.running_mean, m.running_var, m.momentum)\n",
    "    o = m(input1)\n",
    "print(\"after many batches and before call eval(), running_mean and running_var :\", m.running_mean, m.running_var, m.momentum)\n",
    "m.eval()\n",
    "print(\"after many batches and after call eval(), running_mean and running_var :\", m.running_mean, m.running_var, m.momentum)\n",
    "\n",
    "# output: \n",
    "# after many batches and before call eval(), running_mean and running_var : tensor([1.5000, 1.9999]) tensor([0.2857, 1.1429]) 0.1\n",
    "# after many batches and after call eval(), running_mean and running_var : tensor([1.5000, 1.9999]) tensor([0.2857, 1.1429]) 0.1\n",
    "\n",
    "input1 = torch.tensor(\n",
    "    [[[[1., 1.],\n",
    "       [1., 1.]],\n",
    "      [[2., 2.],\n",
    "       [2., 2.]]]])\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "print(\"BatchNorm2d eval input1: \", m(input1))\n",
    "\n",
    "# output, note: (1.0 - 1.5) / ((0.2857) ** 0.5) == -0.9353\n",
    "# tensor([[[[    -0.9353,     -0.9353],\n",
    "#           [    -0.9353,     -0.9353]],\n",
    "\n",
    "#          [[     0.0000,      0.0000],\n",
    "#           [     0.0000,      0.0000]]]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2, 2])\n",
      "tensor([0., 0.]) tensor([1., 1.]) 0.1\n",
      "tensor([0.1500, 0.2000]) tensor([0.9286, 1.0143]) 0.1\n",
      "tensor([0.2850, 0.3800]) tensor([0.8643, 1.0271]) 0.1\n",
      "tensor([0.4065, 0.5420]) tensor([0.8064, 1.0387]) 0.1\n",
      "tensor([0.5159, 0.6878]) tensor([0.7544, 1.0491]) 0.1\n",
      "tensor([0.6143, 0.8190]) tensor([0.7075, 1.0585]) 0.1\n",
      "tensor([0.7028, 0.9371]) tensor([0.6653, 1.0669]) 0.1\n",
      "tensor([0.7826, 1.0434]) tensor([0.6274, 1.0745]) 0.1\n",
      "tensor([0.8543, 1.1391]) tensor([0.5932, 1.0814]) 0.1\n",
      "tensor([0.9189, 1.2252]) tensor([0.5624, 1.0875]) 0.1\n",
      "tensor([0.9770, 1.3026]) tensor([0.5348, 1.0930]) 0.1\n",
      "tensor([1.0293, 1.3724]) tensor([0.5099, 1.0980]) 0.1\n",
      "tensor([1.0764, 1.4351]) tensor([0.4874, 1.1025]) 0.1\n",
      "tensor([1.1187, 1.4916]) tensor([0.4673, 1.1065]) 0.1\n",
      "tensor([1.1568, 1.5425]) tensor([0.4491, 1.1102]) 0.1\n",
      "tensor([1.1912, 1.5882]) tensor([0.4328, 1.1134]) 0.1\n",
      "tensor([1.2220, 1.6294]) tensor([0.4181, 1.1164]) 0.1\n",
      "tensor([1.2498, 1.6665]) tensor([0.4048, 1.1190]) 0.1\n",
      "tensor([1.2749, 1.6998]) tensor([0.3929, 1.1214]) 0.1\n",
      "tensor([1.2974, 1.7298]) tensor([0.3822, 1.1236]) 0.1\n",
      "tensor([1.3176, 1.7568]) tensor([0.3726, 1.1255]) 0.1\n",
      "tensor([1.3359, 1.7812]) tensor([0.3639, 1.1272]) 0.1\n",
      "tensor([1.3523, 1.8030]) tensor([0.3561, 1.1288]) 0.1\n",
      "tensor([1.3671, 1.8227]) tensor([0.3490, 1.1302]) 0.1\n",
      "tensor([1.3804, 1.8405]) tensor([0.3427, 1.1315]) 0.1\n",
      "tensor([1.3923, 1.8564]) tensor([0.3370, 1.1326]) 0.1\n",
      "tensor([1.4031, 1.8708]) tensor([0.3319, 1.1336]) 0.1\n",
      "tensor([1.4128, 1.8837]) tensor([0.3272, 1.1345]) 0.1\n",
      "tensor([1.4215, 1.8953]) tensor([0.3231, 1.1354]) 0.1\n",
      "tensor([1.4293, 1.9058]) tensor([0.3194, 1.1361]) 0.1\n",
      "tensor([1.4364, 1.9152]) tensor([0.3160, 1.1368]) 0.1\n",
      "tensor([1.4428, 1.9237]) tensor([0.3130, 1.1374]) 0.1\n",
      "tensor([1.4485, 1.9313]) tensor([0.3102, 1.1380]) 0.1\n",
      "tensor([1.4536, 1.9382]) tensor([0.3078, 1.1384]) 0.1\n",
      "tensor([1.4583, 1.9444]) tensor([0.3056, 1.1389]) 0.1\n",
      "tensor([1.4625, 1.9499]) tensor([0.3036, 1.1393]) 0.1\n",
      "tensor([1.4662, 1.9549]) tensor([0.3018, 1.1396]) 0.1\n",
      "tensor([1.4696, 1.9594]) tensor([0.3002, 1.1400]) 0.1\n",
      "tensor([1.4726, 1.9635]) tensor([0.2987, 1.1403]) 0.1\n",
      "tensor([1.4754, 1.9672]) tensor([0.2974, 1.1405]) 0.1\n",
      "tensor([1.4778, 1.9704]) tensor([0.2963, 1.1407]) 0.1\n",
      "tensor([1.4800, 1.9734]) tensor([0.2952, 1.1410]) 0.1\n",
      "tensor([1.4820, 1.9761]) tensor([0.2943, 1.1411]) 0.1\n",
      "tensor([1.4838, 1.9784]) tensor([0.2934, 1.1413]) 0.1\n",
      "tensor([1.4855, 1.9806]) tensor([0.2926, 1.1415]) 0.1\n",
      "tensor([1.4869, 1.9825]) tensor([0.2919, 1.1416]) 0.1\n",
      "tensor([1.4882, 1.9843]) tensor([0.2913, 1.1417]) 0.1\n",
      "tensor([1.4894, 1.9859]) tensor([0.2908, 1.1418]) 0.1\n",
      "tensor([1.4905, 1.9873]) tensor([0.2903, 1.1419]) 0.1\n",
      "tensor([1.4914, 1.9885]) tensor([0.2898, 1.1420]) 0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "m = nn.BatchNorm2d(2, affine = False, momentum = 0.1, track_running_stats = True)\n",
    "# input1.shape (2, 2, 2, 2)\n",
    "input1 = torch.tensor(\n",
    "    [[[[1., 1.],\n",
    "       [1., 1.]],\n",
    "      [[1., 1.],\n",
    "       [1., 1.]]], \n",
    "     [[[2., 2.], \n",
    "       [2., 2.]], \n",
    "      [[3., 3.], \n",
    "       [3., 3.]]]])\n",
    "print(input1.shape)\n",
    "\n",
    "for i in range(50):\n",
    "    print(m.running_mean, m.running_var, m.momentum)\n",
    "    m(input1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2, 2])\n",
      "tensor([0., 0.]) tensor([1., 1.]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n",
      "tensor([1.5000, 2.0000]) tensor([0.2857, 1.1429]) 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "m = nn.BatchNorm2d(2, affine = False, momentum = 1, track_running_stats = True)\n",
    "# input1.shape (2, 2, 2, 2)\n",
    "input1 = torch.tensor(\n",
    "    [[[[1., 1.],\n",
    "       [1., 1.]],\n",
    "      [[1., 1.],\n",
    "       [1., 1.]]], \n",
    "     [[[2., 2.], \n",
    "       [2., 2.]], \n",
    "      [[3., 3.], \n",
    "       [3., 3.]]]])\n",
    "print(input1.shape)\n",
    "\n",
    "for i in range(100):\n",
    "    print(m.running_mean, m.running_var, m.momentum)\n",
    "    m(input1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2, 2])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n",
      "tensor([[[[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "m = nn.BatchNorm2d(2, affine = False, momentum = 1, track_running_stats = False)\n",
    "m.running_mean = nn.Parameter(torch.tensor([0.0, 0.0], requires_grad = False))\n",
    "m.running_var = nn.Parameter(torch.tensor([0.5, 0.5], requires_grad = False))\n",
    "# input1.shape (2, 2, 2, 2)\n",
    "input1 = torch.tensor(\n",
    "    [[[[1., 1.],\n",
    "       [1., 1.]],\n",
    "      [[1., 1.],\n",
    "       [1., 1.]]], \n",
    "     [[[2., 2.], \n",
    "       [2., 2.]], \n",
    "      [[3., 3.], \n",
    "       [3., 3.]]]])\n",
    "print(input1.shape)\n",
    "\n",
    "for i in range(10):\n",
    "    # print(m.running_mean, m.running_var, m.momentum)\n",
    "    print(m(input1))\n",
    "\n",
    "print(m(input1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('openmmlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c745df823b0b8fbc96bd327094d24a497ca88aeabc85de830d0531f0a8d26eb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
