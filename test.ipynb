{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/notes/autograd.html\n",
    "\n",
    "How autograd encodes the history\n",
    "Autograd is reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "Internally, autograd represents this graph as a graph of Function objects (really expressions), which can be apply() ed to compute the result of evaluating the graph. When computing the forwards pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the .grad_fn attribute of each torch.Tensor is an entry point into this graph). When the forwards pass is completed, we evaluate this graph in the backwards pass to compute the gradients.\n",
    "\n",
    "An important thing to note is that the graph is recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate.\n",
    "\n",
    "\n",
    "Setting requires_grad\n",
    "requires_grad is a flag, defaulting to false unless wrapped in a nn.Parameter, that allows for fine-grained exclusion of subgraphs from gradient computation. It takes effect in both the forward and backward passes:\n",
    "\n",
    "During the forward pass, an operation is only recorded in the backward graph if at least one of its input tensors require grad. During the backward pass (.backward()), only leaf tensors with requires_grad=True will have gradients accumulated into their .grad fields.\n",
    "\n",
    "It is important to note that even though every tensor has this flag, setting it only makes sense for leaf tensors (tensors that do not have a grad_fn, e.g., a nn.Module’s parameters). Non-leaf tensors (tensors that do have grad_fn) are tensors that have a backward graph associated with them. Thus their gradients will be needed as an intermediary result to compute the gradient for a leaf tensor that requires grad. **From this definition, it is clear that all non-leaf tensors will automatically have require_grad=True.**  \n",
    "\n",
    "上面的这句话是错误的。 正确的应该是，在最终的生成的有向无环图上，如果某个中间节点，对应的所有的leaf tensor 或者 其他上游tensor都是 require_grad == false，那么该 non-leaf tensor的 require_grad 也会默认是 false。 例如可以参考 下面的例子。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6190,  0.3973,  0.2645, -0.0750,  0.6133], requires_grad=True)\n",
      "x1 requires_grad: True\n",
      "x2 requires_grad: False\n",
      "y1 requires_grad: True\n",
      "y2 requires_grad: False\n",
      "z requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.randn(5, requires_grad=True)\n",
    "print(x1)\n",
    "\n",
    "x2 = torch.randn(5)\n",
    "\n",
    "y1 = x1.pow(x1)\n",
    "\n",
    "y2 = x2.pow(x2)\n",
    "\n",
    "z = y1 + y2\n",
    "\n",
    "print(\"x1 requires_grad:\", x1.requires_grad)\n",
    "print(\"x2 requires_grad:\", x2.requires_grad)\n",
    "print(\"y1 requires_grad:\", y1.requires_grad)\n",
    "print(\"y2 requires_grad:\", y2.requires_grad)\n",
    "print(\"z requires_grad:\", z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1747]) torch.Size([1])\n",
      "tensor(0.1747) torch.Size([])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'float'>\n",
      "tensor(1.1747) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.randn(1)\n",
    "print(a, a.shape)\n",
    "a = a.squeeze()\n",
    "print(a, a.shape)\n",
    "\n",
    "print(type(a))\n",
    "\n",
    "x = 1.0\n",
    "print(type(x))\n",
    "\n",
    "x += a\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training  is being added into this instance\n",
      "_parameters  is being added into this instance\n",
      "_buffers  is being added into this instance\n",
      "_non_persistent_buffers_set  is being added into this instance\n",
      "_backward_hooks  is being added into this instance\n",
      "_is_full_backward_hook  is being added into this instance\n",
      "_forward_hooks  is being added into this instance\n",
      "_forward_pre_hooks  is being added into this instance\n",
      "_state_dict_hooks  is being added into this instance\n",
      "_load_state_dict_pre_hooks  is being added into this instance\n",
      "_load_state_dict_post_hooks  is being added into this instance\n",
      "_modules  is being added into this instance\n",
      "weight  is being added into this instance\n",
      "bias  is being added into this instance\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/dev/pytorch-learn/test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B120.92.68.99/root/dev/pytorch-learn/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(name, \u001b[39m\"\u001b[39m\u001b[39m is being added into this instance\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B120.92.68.99/root/dev/pytorch-learn/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m my \u001b[39m=\u001b[39m MyLinear(\u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B120.92.68.99/root/dev/pytorch-learn/test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39;49m(my\u001b[39m.\u001b[39;49mparameters()))\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Union, Tuple, Any, Callable, Iterator, Set, Optional, overload, TypeVar, Mapping, Dict, List\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "  def __init__(self, in_features, out_features):\n",
    "    super().__init__()\n",
    "    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
    "    self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "  def forward(self, input):\n",
    "    return (input @ self.weight) + self.bias\n",
    "\n",
    "  def register_module(self, name: str, module: Optional['Module']) -> None:\n",
    "    r\"\"\"Alias for :func:`add_module`.\"\"\"\n",
    "    self.add_module(name, module)\n",
    "  def __setattr__(self, name, value):\n",
    "    super().__setattr__(name, value)\n",
    "    print(name, \" is being added into this instance\")\n",
    "\n",
    "my = MyLinear(3, 64)\n",
    "print(len(list(my.parameters())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch.1.12-learn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "708194e2fd8b4e9ba6b18dc29c74095066ab8fe23c9bceb551782d7664d0dcab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
